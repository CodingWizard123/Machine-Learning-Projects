{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Decision Tree Classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spam Kaggle Competition: https://www.kaggle.com/competitions/cs189-hw5-spam-spring-2024\n",
    "\n",
    "Titanic Kaggle Competition: https://www.kaggle.com/competitions/cs189-hw5-titanic-spring-2024\n",
    "\n",
    "Objective: \n",
    "\n",
    "    Implement decision trees and random forests for classification. \n",
    "\n",
    "    Titanic:\n",
    "        Predict survivors\n",
    "    Spam:\n",
    "        Predict if email is spam or not \n",
    "\n",
    "command to print tree: dot -Tpdf custom_tree.dot -o custom_tree.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Classifier Accuracy for spam:  0.8268206039076377\n",
      "Random Forest Accuracy for spam:  0.8348134991119005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import io\n",
    "\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "\n",
    "def resultsToCsv(y_test, dataset):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1 # Ensures that the index starts at 1\n",
    "    df.to_csv(f\"{dataset}_predictions.csv\", index_label='Id')\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=None, feature_labels=None, class_names=None, max_features=False, random_state = None):\n",
    "        # model hyperparameters\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.class_names = class_names\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if random_state is not None:\n",
    "            self.rs = np.random.RandomState(seed=self.random_state)\n",
    "        else:\n",
    "            self.rs = np.random.RandomState()\n",
    "\n",
    "        # model data and variables\n",
    "        self.data, self.pred = None, None\n",
    "        self.root = None\n",
    "        self.y = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self):\n",
    "            self.split_rule = None\n",
    "            self.right = None\n",
    "            self.left = None\n",
    "            self.label = None\n",
    "\n",
    "    def entropy(self, y):\n",
    "        # check for division 0\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        # calculate posterior probability \n",
    "        p_c =  np.sum(y) / len(y)\n",
    "        # if posterior probability is 0 or 1 return 0 entropy\n",
    "        if p_c in [0, 1]:\n",
    "            return 0\n",
    "        p_nc = 1  - p_c\n",
    "        return -(p_c * np.log2(p_c) + p_nc * np.log2(p_nc))\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        \"\"\"\n",
    "        Calls split_test to perform split on the feature  and threshold\n",
    "            i.e. split_test(self, X, pclass=1, 1.5)\n",
    "        X: feature matrix\n",
    "        y: prediction labels\n",
    "        idx: the index of the feature {0,1,2....14} for titanic\n",
    "        thresh: threshold to use for split\n",
    "        \"\"\"\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        \"\"\"Returns two substers of X based on feature and threshold\n",
    "        and the indices of the samples idx0, idx1,\n",
    "        It then uses these indices to split the label vector y into two parts (y0, y1)\n",
    "        It then returns the split matrices and their indices\n",
    "        \"\"\"\n",
    "        idx0 = np.where(X[:, idx] <= thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] > thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "    \n",
    "    def build_tree(self, X, y, j):\n",
    "\n",
    "        # some variables\n",
    "        entropy_before = self.entropy(y)\n",
    "        best_info_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # Change this so we split on random features\n",
    "        if self.max_features == False:\n",
    "            indexes = np.arange(X.shape[1])\n",
    "        elif self.max_features == True:\n",
    "            indexes = self.rs.choice(range(X.shape[1]), size=M, replace=False)\n",
    "        \n",
    "        # iterate through the columns\n",
    "        #  [b'pclass', b'age', b'sibsp', b'parch', b'fare', b'male', b'female', b'S', b'C', b'Q']\n",
    "        for i in indexes:\n",
    "            # initial thresholds are unique categores\n",
    "            thresholds = np.unique(X[:,i])\n",
    "            # if thresholds are continous then use percentile\n",
    "            if thresholds.shape[0] > 12:\n",
    "                thresholds = [np.quantile(X[:,i], .55)]\n",
    "            # iterate through the thresholds (only one if a percentile)\n",
    "            for t in thresholds:\n",
    "                # make split at the feature and threshold\n",
    "                x0, y0, x1, y1 = self.split(X,y, i, t)\n",
    "                # \n",
    "                if len(y0) > 0 and len(y1) > 0:\n",
    "                    posterior_x0, posterior_x1 = (len(y0) / len(y)), (len(y1) / len(y))\n",
    "                    entropy_after = posterior_x0 * self.entropy(y0)  + posterior_x1 * self.entropy(y1)\n",
    "                    info_gain = entropy_before - entropy_after\n",
    "                    if info_gain > best_info_gain:\n",
    "                        best_info_gain = info_gain\n",
    "                        best_feature = i\n",
    "                        best_threshold = t\n",
    "                        best_x0, best_x1, best_y0, best_y1 = x0, x1, y0, y1\n",
    "\n",
    "\n",
    "        if best_info_gain <= eps or j == self.max_depth:\n",
    "            n = self.Node()\n",
    "            n.label = np.bincount(y).argmax()\n",
    "            return n\n",
    "        else:\n",
    "            node = self.Node()\n",
    "            node.split_rule = (best_feature, best_threshold)\n",
    "            node.left = self.build_tree(best_x0,best_y0, j+1)\n",
    "            node.right = self.build_tree(best_x1,best_y1, j+1)\n",
    "            return node\n",
    "\n",
    "    def __repr__(self):\n",
    "        def recurse(node, depth):\n",
    "            if node is None:\n",
    "                return \" \" * depth + \"None\\n\"\n",
    "                \n",
    "            if node.label is not None:\n",
    "                return \" \" * depth + f\"Predict: {node.label}\\n\"\n",
    "            \n",
    "            feature_name = node.split_rule[0]\n",
    "            threshold = node.split_rule[1]\n",
    "            left_repr = recurse(node.left, depth + 1) if node.left else \" \" * (depth + 1) + \"None\\n\"\n",
    "            right_repr = recurse(node.right, depth + 1) if node.right else \" \" * (depth + 1) + \"None\\n\"\n",
    "            \n",
    "            node_repr = (\"   \" * depth + f\"If {feature_name} <= {threshold}:\\n\" + left_repr +\n",
    "                 \" \" * depth + f\"Else {feature_name} > {threshold}:\\n\" + right_repr)\n",
    "            \n",
    "            return node_repr\n",
    "\n",
    "        if self.root == None:\n",
    "            return \"empty tree\"\n",
    "        else:\n",
    "            return recurse(self.root, 0)\n",
    "\n",
    "    def viz(self):\n",
    "        with open('custom_tree.dot', 'w') as f:\n",
    "            self.export_tree(self.root, self.class_names,f)\n",
    "\n",
    "\n",
    "    def export_tree(self, node, class_names, dot_file):\n",
    "        def recurse(node, parent_id, next_node_id):\n",
    "            if node is None:\n",
    "                return next_node_id\n",
    "        \n",
    "            node_id = next_node_id\n",
    "            \n",
    "            if node.label is not None:\n",
    "                label = class_names[node.label]\n",
    "                dot_file.write(f'{node_id} [label=\"{label}\", shape=\"box\"];\\n')\n",
    "            else:\n",
    "                feature_name = self.features[node.split_rule[0]]\n",
    "                label = f'{feature_name} <= {node.split_rule[1]}'\n",
    "                dot_file.write(f'{node_id} [label=\"{label}\"];\\n')\n",
    "\n",
    "            next_node_id += 1\n",
    "\n",
    "            if node.left is not None:\n",
    "                dot_file.write(f'{node_id} -> {next_node_id} [label=\"true\"];\\n')\n",
    "                next_node_id = recurse(node.left, node_id, next_node_id)\n",
    "            if node.right is not None:\n",
    "                dot_file.write(f'{node_id} -> {next_node_id} [label=\"false\"];\\n')\n",
    "                next_node_id = recurse(node.right, node_id, next_node_id)\n",
    "        \n",
    "            return next_node_id\n",
    "        \n",
    "        dot_file.write('digraph G {\\n')\n",
    "        dot_file.write('node [shape=ellipse];\\n')\n",
    "        recurse(node, None, 0)\n",
    "        dot_file.write('}\\n')\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.data = X\n",
    "        self.y = y\n",
    "        j = 0\n",
    "        # build tree and initialize the root with the entire tree\n",
    "        self.root = self.build_tree(self.data, self.y, j)\n",
    "        # generate a dot file of the trained tree. To display call the command in first cell block\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Takes a test set and returns the prediction of each row\n",
    "        recursively by calling recurse on the root node, and a row\n",
    "        This gets a label / prediction or calls recurse again.\n",
    "        '''\n",
    "        def rec(n, row):\n",
    "            '''\n",
    "            takes the root of the tree and a row\n",
    "            gows dow the tree and compares the value of the row\n",
    "            at the split index and and splits accordingly\n",
    "            returns the prediction for that row\n",
    "            '''   \n",
    "            if n.label is not None:\n",
    "                return n.label\n",
    "            else:\n",
    "                idx, threshold = n.split_rule\n",
    "                #print(f\"{self.features[idx]}\")\n",
    "                if row[idx] <= threshold:\n",
    "                    return rec(n.left, row)\n",
    "                if row[idx] > threshold:\n",
    "                    return rec(n.right, row)\n",
    "                \n",
    "        # to store predictions\n",
    "        predictions = []\n",
    "\n",
    "        if self.root == None:\n",
    "            return \"!ERROR IN PREDICT!\"\n",
    "        \n",
    "        # make predicitons for each row recursively\n",
    "        for i in range(X.shape[0]):\n",
    "            predictions.append(rec(self.root, X[i]))\n",
    "\n",
    "        # return the prediction made\n",
    "        return predictions\n",
    "\n",
    "class BaggedTrees():\n",
    "    def __init__(self, params=None, n=None):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [DecisionTree(**self.params, random_state=i) for i in range(self.n)]\n",
    "        \n",
    "        # for each tree in decision_tree\n",
    "class RandomForest(BaggedTrees):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, params=None, n=300, m=True):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        # number of trees in forest\n",
    "        self.n = n \n",
    "        super().__init__(params=params, n=self.n) # call the BaggedTrees constructor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Takes the training data and uses the decision trees defined by the construcor\n",
    "        to generate random sample points and train each decision tree\n",
    "        \"\"\"\n",
    "\n",
    "        # for each tree the the constructor defined \n",
    "        for t in self.decision_trees:\n",
    "            # using the random object inside each tree generate sample points and labels\n",
    "            rand_indices = t.rs.choice(range(X.shape[0]),size=X.shape[0], replace=False)\n",
    "            random_points = X[rand_indices]\n",
    "            random_labels = y[rand_indices]\n",
    "            # fit the tree with the random points and labels\n",
    "            t.fit(random_points, random_labels)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get the first trees predictions\n",
    "        predictions = np.array(self.decision_trees[0].predict(X))[:, np.newaxis]\n",
    "\n",
    "        for i in range(1, len(self.decision_trees)):\n",
    "            current_trees_predictions = np.array(self.decision_trees[i].predict(X))[:, np.newaxis]\n",
    "            predictions = np.hstack((predictions, current_trees_predictions))\n",
    "        \n",
    "        final_predictions = scipy.stats.mode(predictions, axis=1)[0].flatten()\n",
    "        return final_predictions\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "\n",
    "    data = np.hstack(\n",
    "        [np.array(data, dtype=float),\n",
    "         np.array(onehot_encoding)])\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        print(\"First splits\", first_splits)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # pick dataset\n",
    "    #dataset = \"titanic\"\n",
    "    dataset = \"spam\"\n",
    "    \n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = 'datasets/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "        path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "        y = data[1:, 0]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "        labeled_idx = np.where(y != b'')[0]\n",
    "        y = np.array(y[labeled_idx], dtype=float).astype(int)\n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "        # Replace -1 wiw nan\n",
    "        X[X == -1] = np.nan\n",
    "        Z[Z == -1] = np.nan\n",
    "\n",
    "        # model parameters\n",
    "        eps = 1e-2 # determines information gain stopping criterion \n",
    "        M = 11 # determines how many random features to use in Random Forest taking into account that 4 of the columns are 0\n",
    "        w = 'uniform'   # KNN parameter\n",
    "        \n",
    "        # impute training data\n",
    "        imputer = KNNImputer(n_neighbors=5, weights = w)\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "        # impute test data\n",
    "        test_imputer = KNNImputer(n_neighbors=5, weights=w)\n",
    "        Z_imputed = test_imputer.fit_transform(Z)\n",
    "\n",
    "        # Split data \n",
    "        shuffled_indices = np.random.permutation(len(X_imputed))\n",
    "        split_index = int(len(X_imputed) * 0.80)\n",
    "        X_train = X_imputed[shuffled_indices][:split_index]\n",
    "        Y_train = y[shuffled_indices][:split_index].astype(int)\n",
    "        X_valid = X_imputed[shuffled_indices][split_index:]\n",
    "        Y_valid = y[shuffled_indices][split_index:].astype(int)\n",
    "\n",
    "        # Random Forest Parameters\n",
    "        params = {\n",
    "            \"max_depth\": 25,\n",
    "            \"feature_labels\": features,\n",
    "            \"class_names\": class_names,\n",
    "            }\n",
    "        \n",
    "        # Decision Tree\n",
    "        myTree = DecisionTree(max_depth=25,feature_labels=features, class_names=class_names)\n",
    "        myTree.fit(X_train, Y_train)\n",
    "        my_classifiers_predictions = myTree.predict(X_valid)\n",
    "        accuracy = np.sum(my_classifiers_predictions == Y_valid) / len(Y_valid)\n",
    "        print(f\"Tree Classifier Accuracy for {dataset}:  {accuracy}\")\n",
    "        test_predictions = myTree.predict(Z_imputed)\n",
    "        # export predictions\n",
    "        #resultsToCsv(np.array(test_predictions), dataset)\n",
    "        myTree.viz()\n",
    "        \n",
    "        # Random Forest\n",
    "        random_forest = RandomForest(params)\n",
    "        random_forest.fit(X_train, Y_train)\n",
    "        random_forest_validation_predictions = random_forest.predict(X_valid)\n",
    "        print(\"Random Forest Accuracy: \", sum(random_forest_validation_predictions == Y_valid)  / len(Y_valid))\n",
    "        random_forest_test_predictions = random_forest.predict(Z_imputed)\n",
    "        resultsToCsv(random_forest_test_predictions, f\"{dataset}_random_forest\")\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        \n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        eps = 1e-3 # determines information gain stopping criterion \n",
    "        M = 7 # determines how many random features to use in Random Forest taking into account that 4 of the columns are 0\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam_data/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "        params = {\n",
    "        \"max_depth\": 15,\n",
    "        \"feature_labels\": features,\n",
    "        \"class_names\": class_names\n",
    "        }\n",
    "        # shuffle the training data \n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        split_index = int(len(X) * 0.80)\n",
    "\n",
    "\n",
    "        # split the training data into a train and validation set \n",
    "        X_train = X[shuffled_indices][:split_index]\n",
    "        Y_train = y[shuffled_indices][:split_index].astype(int)\n",
    "        X_valid = X[shuffled_indices][split_index:]\n",
    "        Y_valid = y[shuffled_indices][split_index:].astype(int)\n",
    "\n",
    "        # Decision Tree\n",
    "        myTree = DecisionTree(max_depth=15,feature_labels=features, class_names=class_names)\n",
    "        myTree.fit(X_train, Y_train)\n",
    "        my_classifiers_predictions = myTree.predict(X_valid)\n",
    "        accuracy = np.sum(my_classifiers_predictions == Y_valid) / len(Y_valid)\n",
    "        print(f\"Tree Classifier Accuracy for {dataset}:  {accuracy}\")\n",
    "        test_predictions = myTree.predict(Z)\n",
    "        \n",
    "        # export predictions\n",
    "        resultsToCsv(np.array(test_predictions), dataset)\n",
    "\n",
    "        # Random Forest\n",
    "        random_forest = RandomForest(params)\n",
    "        random_forest.fit(X_train, Y_train)\n",
    "        random_forest_validation_predictions = random_forest.predict(X_valid)\n",
    "        randomForestAccuracy = sum(random_forest_validation_predictions == Y_valid)  / len(Y_valid)\n",
    "        print(f\"Random Forest Accuracy for {dataset}: \", randomForestAccuracy)\n",
    "        random_forest_test_predictions = random_forest.predict(Z)\n",
    "\n",
    "        resultsToCsv(random_forest_test_predictions, f\"{dataset}_random_forest\")\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
